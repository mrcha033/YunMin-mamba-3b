# YunMin-Mamba 3B - SageMaker Training

ì´ í”„ë¡œì íŠ¸ëŠ” Amazon SageMakerì—ì„œ YunMin-Mamba 3B ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•œ Docker ì´ë¯¸ì§€ì™€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ—ï¸ Architecture

- **Model**: Mamba-based Language Model (2.78B parameters)
- **Tokenizer**: YunMin-tokenizer-96k (96,000 vocab size)
- **Training**: Multi-category curriculum learning
- **Platform**: Amazon SageMaker Training Jobs

## ğŸ“¦ Files Overview

- `Dockerfile`: SageMaker í˜¸í™˜ Docker ì´ë¯¸ì§€
- `train_mamba.py`: ë©”ì¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
- `build_and_push_ecr.ps1`: ECR í‘¸ì‹œ ìŠ¤í¬ë¦½íŠ¸ (Windows PowerShell ì˜ˆì‹œ)
- `sagemaker_spot_training_job.py`: Spot ì¸ìŠ¤í„´ìŠ¤ìš© í›ˆë ¨ ì‘ì—… ìŠ¤í¬ë¦½íŠ¸
- `sagemaker_training_job.py`: SageMaker í›ˆë ¨ ì‘ì—… ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- `requirements.txt`: Python ì˜ì¡´ì„±
- `accelerate_config.yaml`: Hugging Face Accelerate ì„¤ì •

## ğŸš€ Quick Start

### 1. ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ

```bash
# AWS CLI ì„¤ì • í™•ì¸
aws sts get-caller-identity

# ECRì— ì´ë¯¸ì§€ ë¹Œë“œ ë° í‘¸ì‹œ (Windows PowerShell ê¸°ì¤€)
./build_and_push_ecr.ps1
```

### 2. ë°ì´í„° ì¤€ë¹„

S3ì— ë‹¤ìŒ êµ¬ì¡°ë¡œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”:

```
s3://your-bucket/yunmin-mamba-data/dataset/tagged/
â”œâ”€â”€ main_data/
â”‚   â”œâ”€â”€ shard_001/
â”‚   â”œâ”€â”€ shard_002/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ korean_textbook/
â”œâ”€â”€ academic_data/
â”œâ”€â”€ papers_data/
â”œâ”€â”€ national_data/
â”œâ”€â”€ national_assembly_data/
â”œâ”€â”€ web_data/
â””â”€â”€ social_media_data/
```

### 3. SageMaker í›ˆë ¨ ì‘ì—… ì‹œì‘

```python
# sagemaker_training_job.py ìˆ˜ì • í›„ ì‹¤í–‰
python sagemaker_training_job.py
```

## âš™ï¸ Configuration

### Hyperparameters

SageMaker í›ˆë ¨ ì‘ì—…ì—ì„œ ë‹¤ìŒ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `learning_rate`: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 5e-5)
- `batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 8)
- `num_workers`: ë°ì´í„° ë¡œë” ì›Œì»¤ ìˆ˜ (ê¸°ë³¸ê°’: 4)
- `save_steps`: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê°„ê²© (ê¸°ë³¸ê°’: 1000)

### Instance Types

ê¶Œì¥ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…:

- **ëŒ€ê·œëª¨ í›ˆë ¨**: `ml.p4d.24xlarge` (8x A100 40GB)
- **ì¤‘ê°„ ê·œëª¨**: `ml.p3.8xlarge` (4x V100 16GB)
- **ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸**: `ml.g4dn.xlarge` (1x T4 16GB)

### Model Configuration

`mamba_config.json`:
```json
{
  "vocab_size": 96000,
  "d_model": 2560,
  "num_hidden_layers": 64,
  "model_type": "mamba"
}
```

## ğŸ“Š Learning Order

ëª¨ë¸ì€ ë‹¤ìŒ ìˆœì„œë¡œ ë°ì´í„°ì…‹ ì¹´í…Œê³ ë¦¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤:

1. **main_data**: ëŒ€ê·œëª¨ í•œêµ­ì–´ ë§ë­‰ì¹˜ - ê¸°ë³¸ ì–¸ì–´ ëŠ¥ë ¥
2. **korean_textbook**: ì •í˜•í™”ëœ ë¬¸ì²´ì™€ ë¬¸ë²• - ì–¸ì–´ ê·œì¹™ì„±
3. **academic_data**: ì „ë¬¸ ì–´íœ˜ì™€ ë¬¸ì¥ - í‘œí˜„ë ¥ í™•ì¥
4. **papers_data**: ì—°êµ¬ ë…¼ë¬¸ - ê³ ê¸‰ ì–´íœ˜ì™€ ë³µì¡í•œ êµ¬ì¡°
5. **national_data**: í–‰ì • ë¬¸ì„œ - ê³µì‹ì  í‘œí˜„ê³¼ ìš©ì–´
6. **national_assembly_data**: êµ­íšŒ ë°œì–¸ë¡ - êµ¬ì–´ì²´ì™€ ë…¼ë¦¬ì  í‘œí˜„
7. **web_data**: ì›¹ í…ìŠ¤íŠ¸ - ë¹„ì •í˜• í‘œí˜„ê³¼ ìµœì‹  ìš©ì–´
8. **social_media_data**: SNS ë°ì´í„° - ì¼ìƒ ëŒ€í™”ì²´ì™€ ì‹ ì¡°ì–´

## ğŸ”§ SageMaker Environment Variables

ì»¨í…Œì´ë„ˆëŠ” ë‹¤ìŒ SageMaker í™˜ê²½ ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤:

- `SM_CHANNEL_TRAINING`: í›ˆë ¨ ë°ì´í„° ê²½ë¡œ
- `SM_MODEL_DIR`: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
- `SM_OUTPUT_DATA_DIR`: ì¶œë ¥ ë°ì´í„° ê²½ë¡œ
- `/opt/ml/input/config/hyperparameters.json`: í•˜ì´í¼íŒŒë¼ë¯¸í„°

## ğŸ“ Output Structure

í›ˆë ¨ ì™„ë£Œ í›„ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ê²°ê³¼ê°€ ì €ì¥ë©ë‹ˆë‹¤:

```
/opt/ml/model/
â”œâ”€â”€ final/
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ tokenizer files...
â”œâ”€â”€ step-1000-main_data/
â”œâ”€â”€ step-2000-main_data/
â””â”€â”€ ...

/opt/ml/output/
â”œâ”€â”€ training.log
â””â”€â”€ logs/
    â””â”€â”€ train.log
```

## ğŸ³ Docker Commands

### ë¡œì»¬ í…ŒìŠ¤íŠ¸

```bash
# ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
docker run --rm \
  -v /path/to/local/data:/opt/ml/input/data/training \
  -v /path/to/output:/opt/ml/model \
  yunmin-mamba:latest

# GPUê°€ ìˆëŠ” ê²½ìš°
docker run --rm --gpus all \
  -v /path/to/local/data:/opt/ml/input/data/training \
  -v /path/to/output:/opt/ml/model \
  yunmin-mamba:latest
```

### ë””ë²„ê¹…

```bash
# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
docker run -it --rm yunmin-mamba:latest bash

# ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
docker run --rm yunmin-mamba:latest python -c "
from transformers import AutoConfig, AutoModelForCausalLM
AutoModelForCausalLM.from_config(AutoConfig.from_pretrained('mamba_config.json'))
print('âœ… Model can be imported')
"
```

## ğŸ“‹ Monitoring

### CloudWatch Logs

SageMaker ì½˜ì†”ì—ì„œ ë‹¤ìŒ ë¡œê·¸ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- `/aws/sagemaker/TrainingJobs`: í›ˆë ¨ ë¡œê·¸
- Training jobì˜ CloudWatch ë§í¬ì—ì„œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

### í›ˆë ¨ ì§„í–‰ë¥ 

ë¡œê·¸ì—ì„œ ë‹¤ìŒ ë©”íŠ¸ë¦­ì„ í™•ì¸í•˜ì„¸ìš”:

- ê° ì¹´í…Œê³ ë¦¬ë³„ í›ˆë ¨ ì§„í–‰ë¥ 
- 50ìŠ¤í…ë§ˆë‹¤ loss ì¶œë ¥
- 1000ìŠ¤í…ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

## ğŸ› ï¸ Troubleshooting

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **CUDA Out of Memory**
   - `batch_size` ì¤„ì´ê¸°
   - `num_workers` ì¤„ì´ê¸°
   - ë” í° ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì‚¬ìš©

2. **ë°ì´í„° ë¡œë”© ì‹¤íŒ¨**
   - S3 ê¶Œí•œ í™•ì¸
   - ë°ì´í„° ê²½ë¡œ í™•ì¸
   - ë°ì´í„° í˜•ì‹ ê²€ì¦

3. **í›ˆë ¨ ì†ë„ ëŠë¦¼**
   - GPU ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš© í™•ì¸
   - `num_workers` ì¡°ì •
   - ë°°ì¹˜ í¬ê¸° ìµœì í™”

### ë¡œê·¸ í™•ì¸

```bash
# SageMaker í›ˆë ¨ ì‘ì—… ìƒíƒœ í™•ì¸
aws sagemaker describe-training-job --training-job-name your-job-name

# CloudWatch ë¡œê·¸ í™•ì¸
aws logs get-log-events \
  --log-group-name /aws/sagemaker/TrainingJobs \
  --log-stream-name your-job-name/algo-1-timestamp
```

## ğŸ“ Support

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:

1. ECR ì´ë¯¸ì§€ê°€ ì˜¬ë°”ë¥´ê²Œ í‘¸ì‹œë˜ì—ˆëŠ”ì§€
2. SageMaker IAM ì—­í•  ê¶Œí•œ
3. S3 ë°ì´í„° ê²½ë¡œì™€ ê¶Œí•œ
4. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
5. ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì œí•œ

---

**ECR Repository**: `869935091548.dkr.ecr.us-east-1.amazonaws.com/yunmin-mamba:latest` 
