# YunMin-Mamba 3B EC2 Pretraining Plan

## âœ… Phase 0: EC2 í™˜ê²½ ì„¤ì¹˜ & ê·¸ë£¹ êµ¬ì„±

### Task 0.1 â€” EC2 ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •

* **Instance**: `g5.12xlarge` (or better)
* **OS**: Deep Learning AMI (Ubuntu 20.04) or base Ubuntu 22.04
* **Volume**: 500 GB EBS gp3, Expand + Format
* **Security**: SSH + S3 Access Role

### Task 0.2 â€” CUDA & PyTorch ì†Œí™˜ì„±

* CUDA 12.1 + cuDNN + GCC 11 ê°œë°©
* Conda env: `mamba-env`

```bash
conda create -n mamba-env python=3.10 -y
conda activate mamba-env
```

### Task 0.3 â€” PyTorch + mamba-ssm

```bash
pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 \
            transformers>=4.39.3 tokenizers==0.15.2 \
            huggingface-hub datasets sentencepiece bitsandbytes \
            mamba-ssm==2.2.4 causal-conv1d==1.5.0.post8
```

## ğŸ“š Phase 1: í˜•ì‹ êµ¬ì¡° & íŒŒì¼ êµ¬ì„±

### Task 1.1 â€” ëª©ë¡ êµ¬ì¡°

```
/home/ec2-user/
|â”‚
â”œâ”€â”€ train_mamba.py                 # main trainer script
â”œâ”€â”€ mamba_config.json            # Mamba 3B êµ¬ì„± ì •ì˜
â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ YunMin-tokenizer-96k/   # from HuggingFace
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ tagged/            # Arrow format from s3
â”œâ”€â”€ logs/
    â””â”€â”€ train.log
```

### Task 1.2 â€” í˜•ì‹ ì„¤ì • `mamba_config.json`

* hidden\_dim: 3072
* intermediate\_size: 8192
* num\_hidden\_layers: 30
* vocab\_size: 96000

## ğŸš€ Phase 2: Dataset & Tokenizer Integration

### Task 2.1 â€” ë°ì´í„°ì…‹ S3 -> EC2 ë³µì‚¬

```bash
aws s3 cp s3://yeongjopt-ai-bucket/dataset/tagged/ dataset/tagged/ --recursive
```

### Task 2.2 â€” í† í°ì•„ì´ì € ë¡œë“œ

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("mrcha033/YunMin-tokenizer-96k")
```

### Task 2.3 â€” Dataset Load

```python
from datasets import load_dataset

train_data = load_dataset("arrow", data_dir="dataset/clean-corpus")
```

## ğŸ“ Phase 3: Trainer Setup

### Task 3.1 â€” Data Collator

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)
```

### Task 3.2 â€” TrainingArguments

```python
from transformers import TrainingArguments

args = TrainingArguments(
    output_dir="./checkpoints",
    overwrite_output_dir=True,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    logging_dir="./logs",
    save_strategy="steps",
    save_steps=1000,
    logging_steps=50,
    bf16=True,
    report_to="none",
)
```

### Task 3.3 â€” Mamba Model Init

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_config("mamba_config.json")
```

### Task 3.4 â€” Trainer Run

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_data["train"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()
```

## ğŸŒŸ Phase 4: ê²°ê³¼ ê²€ì‚¬ & Checkpoint

* `checkpoints/` íŒŒì¼ì„ S3ê°€ì§€ ì €ì¥

```bash
aws s3 cp checkpoints/ s3://yeongjopt-ai-bucket/checkpoints/ --recursive
```

* `train.log` ì¢…ê²°ì‹œì  loss ê´€ì°°

## ğŸ“Š ëª©í‘œ ì§€í‘œ (PoC ê¸°ì¤€)

* í† í° PPL < 4.0
* ëª¨ë¸ ì˜¤ë¥˜ ì—†ìŒ (CUDA, init, trainer)
* ì „ì²´ corpus ì œê±°ë¡œ 1 epoch ìˆ˜í–‰
